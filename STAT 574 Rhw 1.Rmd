---
title: "STAT 574 HW 1"
output: pdf_document
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Problem 1

```{r}
##### Problem 1 ##################################
##################################################

hospital <- read.csv("C:/Users/saedw/OneDrive/Desktop/STAT 574 Data Mining/HW1STAT574S23/DATA SETS/hospital_data.csv")



###############################################################################
# a) Split the data into 80% training and 20% testing sets and build a regression tree on the training set with the RSS splitting criterion to model surgery cost. Use all the other variables except medical ID as splitting variables. Apply the cost-complexity pruning algorithm to produce a reasonably-sized tree. Give the graphical output.
###############################################################################

# split into testing and training
set.seed(239076)
sample <- sample(c(TRUE, FALSE), nrow(hospital),
                 replace = TRUE, prob = c(0.8, 0.2))
train <- hospital[sample,]
test <- hospital[!sample,]

# Apply RSS splitting Criterion to model surgery cost
library(rpart)
library(rpart.plot)
a1_tree <- rpart(surgery_cost ~ gender + age + BMI + ASA +
                   surgery_duration_min, data=train,
                 method = "anova", xval=10, cp=0)

printcp(a1_tree)

# plot initial decision tree
rpart.plot(a1_tree, type = 3,
           main="Initial Decision Tree")

# complexity Parameter Table Graph - find number optimal number of leaves
plotcp(a1_tree, minline = TRUE, upper = "size")
  # aprox - 7 splits

# reduced decision tree
a1_RSS <- rpart(surgery_cost ~ gender + age + BMI + ASA +
                   surgery_duration_min, data = train,
                method = "anova", cp=0.0086)

# plot prunned tree
rpart.plot(a1_RSS, type=3,
           main = "Pruned Decision Tree")


###############################################################################
# b) Use the fitted model to predict surgery cost for the testing data. 
# Compute proportions of predicted values within 10%, 15%, and 20% of the 
# observed values. 
###############################################################################

# Compute prediction accuracy for testing data
pred_surg_cost <- predict(a1_RSS, newdata = test)

# accuracy within 10%
accuracy10 <- ifelse(abs(test$surgery_cost - pred_surg_cost) < 
                       0.10*test$surgery_cost, 1, 0)
print(mean(accuracy10))

# accuracy withing 15%
accuracy15 <- ifelse(abs(test$surgery_cost - pred_surg_cost) < 
                       0.15*test$surgery_cost, 1, 0)
print(mean(accuracy15))

# accuracy withing 20%
accuracy20 <- ifelse(abs(test$surgery_cost - pred_surg_cost) < 
                       0.20*test$surgery_cost, 1, 0)
print(mean(accuracy20))


######################################################################################
# (c)	Build a regression tree on the training data based on the CHAID splitting criterion and cost-complexity pruning. Give the graphical output. 
######################################################################################

library(CHAID)
library(dplyr)

# mutate continous variables for CHAID splitting
hospital_cat <- mutate(hospital, age_cat=ntile(age,10),
                       BMI_cat=ntile(BMI,10), 
                       surgery_cost_cat=ntile(surgery_cost,10),
                       surgery_duration_cat=ntile(surgery_duration_min,10))

# split 80% train 20% test for mutated data set
set.seed(239076)
sample2 <- sample(c(TRUE, FALSE), nrow(hospital_cat),
                 replace = TRUE, prob = c(0.8, 0.2))
train_cat <- hospital_cat[sample2,]
test_cat <- hospital_cat[!sample2,]

# Fit regression tree for CHAID splitting
a1_chaid <- chaid(as.factor(surgery_cost_cat) ~ as.factor(gender) + as.factor(age_cat) +
                    as.factor(BMI_cat) + as.factor(ASA) +
                    as.factor(surgery_duration_cat), data = train_cat,
                  control = chaid_control(maxheight = 4))

plot(a1_chaid, type="simple")

# prediction accuracy for testing data
pred_chaid <- as.numeric(predict(a1_chaid, newdata = test_cat))
test_cat <- cbind(test_cat, pred_chaid)

# computing predicted values, mean values per decile in the training set
aggr.data <- aggregate(train_cat$surgery_cost, by=list(train_cat$surgery_cost_cat),
                       FUN=mean)

# combining observed and predicted values in the testing set
aggr.data$pred_chaid <- aggr.data$Group.1
aggr.data$p_median_surgery_cost <- aggr.data$x
test_cat <- left_join(test_cat, aggr.data, by="pred_chaid")


################################################################################
# (d)	Use the fitted CHAID tree to predict surgery cost for the data in the testing set. Compute proportions of predicted values within 10%, 15%, and 20% of the observed values. Which of the two models, RSS or CHAID, give better prediction?
################################################################################

#### CHAID accuracy #####

# accuracy within 10%
chaid_acc10 <- ifelse(abs(test_cat$surgery_cost - test_cat$p_median_surgery_cost)
                      < 0.10*test_cat$surgery_cost, 1, 0)
print(mean(chaid_acc10))

# accuracy within 15%
chaid_acc15 <- ifelse(abs(test_cat$surgery_cost - test_cat$p_median_surgery_cost)
                      < 0.15*test_cat$surgery_cost, 1, 0)
print(mean(chaid_acc15))

# accuracy within 20%
chaid_acc20 <- ifelse(abs(test_cat$surgery_cost - test_cat$p_median_surgery_cost)
                      < 0.20*test_cat$surgery_cost, 1, 0)
print(mean(chaid_acc20))


```





## Problem 2

```{r}
##### Problem 2 ##############################
##############################################

library(dplyr)
library(rpart)
library(rpart.plot)

credit_data <- read.csv("C:/Users/saedw/OneDrive/Desktop/STAT 574 Data Mining/HW1STAT574S23/DATA SETS/card_transdata.csv")

View(credit_data)
###############################################################################
# (a)	Split the data into 80% training and 20% testing sets and build a binary classification tree for fraudulent activity on the training set using the Gini splitting criterion. Prune the tree using the cost-complexity pruning algorithm. Give the graphical output.
###############################################################################


# split data 80% train 20% test
sample3 <- sample(c(TRUE, FALSE), nrow(credit_data),
                  replace=TRUE, prob=c(0.8, 0.2))
train_credit <- credit_data[sample3,]
test_credit <- credit_data[!sample3,]

# fitting tree with gini criterion
a2_gini <- rpart(fraud ~ distance_from_home + distance_from_last_transaction +
                   ratio_to_median_purchase_price + repeat_retailer +
                   used_chip + used_pin_number + online_order, data=credit_data,
                 method = "class", parms = list(split="Gini"), maxdepth=4)

# plot pruned tree (gini)
rpart.plot(a2_gini, type=3, main="Gini Binary Classification Tree")

##############################################################################
# (b)	Compute the prediction accuracy for the training data, using the range of classification thresholds between 0.01 and 0.99. What thresholds correspond to the largest prediction accuracy?
##############################################################################


# compute prediction accuracy for testing data
pred_gini <- predict(a2_gini, test_credit)
test_pred <- cbind(test_credit, pred_gini)

test_pred <- test_pred %>% rename("no" = "0")
test_pred <- test_pred %>% rename("yes" = "1")


tp <- matrix(NA, nrow = nrow(test_pred), ncol = 99)
tn <- matrix(NA, nrow = nrow(test_pred), ncol = 99)

for (i in 1:99) {
  tp[,i] <- ifelse(test_pred$fraud==1 & test_pred$yes > 0.01*i, 1, 0)
  tn[,i] <- ifelse(test_pred$fraud==0 & test_pred$no <= 0.01*i, 1, 0)
}

trueclassrate <- matrix(NA, nrow = 99, ncol = 2)
for(i in 1:99){
  trueclassrate[i,1] <- 0.01*i
  trueclassrate[i,2] <- sum(tp[,i] + tn[,i])/nrow(test_pred)
}

trueclassrateFinal <- trueclassrate[which(trueclassrate[,2]==max(trueclassrate[,2])),]

print(trueclassrateFinal)



###############################################################################
# (c)	Fit the binary classification tree using the entropy splitting criterion and cost-complexity pruning algorithm. Display the tree.
###############################################################################

#FITTING PRUNED BINARY TREE WITH ENTROPY SPLITTING 
c2_entropy <- rpart(fraud ~ distance_from_home + distance_from_last_transaction +
                   ratio_to_median_purchase_price + repeat_retailer +
                   used_chip + used_pin_number + online_order, data=credit_data,
                 method = "class", parms = list(split="entropy"), maxdepth=4)

rpart.plot(c2_entropy, type=3)


##############################################################################
# (d)	Compute the prediction accuracy for the training data, using the range of classification thresholds between 0.01 and 0.99. What thresholds correspond to the largest prediction accuracy? - Entropy tree
##############################################################################


# compute prediction accuracy for testing data
pred_entropy <- predict(c2_entropy, test_credit)
test_pred <- cbind(test_credit, pred_entropy)

test_pred <- test_pred %>% rename("no" = "0")
test_pred <- test_pred %>% rename("yes" = "1")


tp <- matrix(NA, nrow = nrow(test_pred), ncol = 99)
tn <- matrix(NA, nrow = nrow(test_pred), ncol = 99)

for (i in 1:99) {
  tp[,i] <- ifelse(test_pred$fraud==1 & test_pred$yes > 0.01*i, 1, 0)
  tn[,i] <- ifelse(test_pred$fraud==0 & test_pred$no <= 0.01*i, 1, 0)
}

trueclassrate <- matrix(NA, nrow = 99, ncol = 2)
for(i in 1:99){
  trueclassrate[i,1] <- 0.01*i
  trueclassrate[i,2] <- sum(tp[,i] + tn[,i])/nrow(test_pred)
}

trueclassrateFinal <- trueclassrate[which(trueclassrate[,2]==max(trueclassrate[,2])),]

print(trueclassrateFinal)

###############################################################################
# (e)	Fit the binary classification tree using the CHAID splitting criterion and cost-complexity pruning algorithm. Display the tree.
###############################################################################


# fit pruned binary tree with chaid splitting
library(dplyr)
cred_mut <- mutate(credit_data, dist.home.cat=ntile(distance_from_home,10),
                   dist.trans.cat=ntile(distance_from_last_transaction,10),
                   ratio.cat=ntile(ratio_to_median_purchase_price,10))

# split data 80-20 split
sample_chaid <- sample(c(TRUE,FALSE),nrow(cred_mut),
                       replace=TRUE, prob=c(0.8,0.2))

train_chaid <- cred_mut[sample_chaid,]
test_chaid <- cred_mut[!sample_chaid,]


# fit binary classification tree
library(CHAID)
tree.chaid <- chaid(as.factor(fraud) ~ as.factor(dist.home.cat) + 
                      as.factor(dist.trans.cat) + as.factor(ratio.cat) +
                      as.factor(repeat_retailer) + as.factor(used_chip) +
                      as.factor(used_pin_number) + as.factor(online_order),
                    data = cred_mut, control = chaid_control(maxheight = 3))

plot(tree.chaid, type = "simple")


################################################################################
# (f)	Compute the prediction accuracy of the CHAID tree for the training data, using the cut-offs for predicted probability of fraud ranging between 0.01 and 0.99. List the cut-offs that give the maximum prediction accuracy. Which of the three trees (Gini, entropy, or CHAID) produces the largest maximum prediction accuracy?
################################################################################


# compute predictin accuracy for testing data
pred_bin_chaid <- predict(tree.chaid, newdata = test_chaid)
test_new <- cbind(test_chaid, pred_bin_chaid)

truepred <-c()
n <- nrow(test_new)
for (i in 1:n) {
  truepred[i] <- ifelse(test_new$fraud[i]==test_new$pred_bin_chaid[i],1,0)
}

print(truepredrate <- mean(truepred))

```

## Problem 3
```{r}
###############################################################################
# Problem 3. Consider the Gini classification tree built in Problem 2. For the predicted classifications on the training data, 
###############################################################################


### using credit data again


###############################################################################
# (a)	Compute the confusion matrix (the number of true positive, false positive, true negative, and false negative predictions). Use the 0.5 cut-off for predicted probability of fraud.
###############################################################################

# split data 80 20
set.seed(1234567)
sample_3a <- sample(c(TRUE, FALSE), nrow(credit_data),
                    replace=TRUE, prob=c(0.8,0.2))
train_3a <- credit_data[sample_3a,]
test_3a <- credit_data[!sample_3a,]

# fit pruend binary tree with gini splitting
library(rpart)
gini_3a <- rpart(fraud ~ distance_from_home + distance_from_last_transaction +
                   ratio_to_median_purchase_price + repeat_retailer +
                   used_chip + used_pin_number + online_order, data = train_3a,
                 method = "class", parms=list(split="Gini"), maxdepth=4)




#COMPUTING CONFUSION MATRIX AND PERFORMANCE MEASURES FOR TESTING DATA
pred.values<- predict(gini_3a, test_3a)
test<- cbind(test_3a,pred.values)

pred.3a <- predict(gini_3a, test_3a)
test_3a <- cbind(test_3a, pred.3a)

View(test_3a)

test_3a <- test_3a %>% rename("no"="0")
test_3a <- test_3a %>% rename("yes"="1")

tp<- c()
fp<- c()
tn<- c()
fn<- c()

total<- nrow(test_3a)
for (i in 1:total){
  tp[i]<- ifelse(test_3a$yes[i]>0.5 & test_3a$fraud[i]=="1",1,0)
  fp[i]<- ifelse(test_3a$yes[i]>0.5 & test_3a$fraud[i]=="0",1,0)
  tn[i]<- ifelse(test_3a$no[i]>0.5 & test_3a$fraud[i]=="0",1,0)
  fn[i]<- ifelse(test_3a$no[i]>0.5 & test_3a$fraud[i]=="1",1,0)
}

print(tp<- sum(tp))
print(fp<- sum(fp))
print(tn<- sum(tn))
print(fn<- sum(fn))
print(total)

print(accuracy<- (tp+tn)/total)
print(misclassrate<- (fp+fn)/total)
print(sensitivity<- tp/(tp+fn))
print(FNR<- fn/(tp+fn))
print(specificity<- tn/(fp+tn))
print(FPR<- fp/(fp+tn))
print(precision<- tp/(tp+fp))
print(NPV<- tn/(fn+tn))
print(F1score<- 2*tp/(2*tp+fn+fp))


```



## Problem 4
```{r}
################################################################################
# Problem 4. Consider the Gini classification tree built in Problem 2. For the predicted classifications on the training data:
################################################################################


################################################################################
# (a)	Compute prediction accuracy, misclassification rate, sensitivity, and specificity for a range of cut-offs between 0.01 and 0.99. 
################################################################################

# use sample_3a, train_3a, test_3a
library(rpart)
gini_3a <- rpart(fraud ~ distance_from_home + distance_from_last_transaction +
                   ratio_to_median_purchase_price + repeat_retailer +
                   used_chip + used_pin_number + online_order, data = train_3a,
                 method = "class", parms=list(split="Gini"), maxdepth=4)


#COMPUTING CONFUSION MATRIX AND PERFORMANCE MEASURES FOR TESTING DATA
# for range of cutoffs

tpos<- matrix(NA, nrow=nrow(test_3a), ncol=102)
fpos<- matrix(NA, nrow=nrow(test_3a), ncol=102)
tneg<- matrix(NA, nrow=nrow(test_3a), ncol=102)
fneg<- matrix(NA, nrow=nrow(test_3a), ncol=102)

for (i in 0:101) {
  tpos[,i+1]<- ifelse(test_3a$fraud=="1" & test_3a$yes>=0.01*i,1,0)
  fpos[,i+1]<- ifelse(test_3a$fraud=="0" & test_3a$yes>=0.01*i, 1,0)
  tneg[,i+1]<- ifelse(test_3a$fraud=="0" & test_3a$yes<0.01*i,1,0)
  fneg[,i+1]<- ifelse(test_3a$fraud=="1" & test_3a$yes<0.01*i,1,0)
}


tp<- c()
fp<- c()
tn<- c()
fn<- c()
accuracy<- c()
misclassrate<- c()
sensitivity<- c()
specificity<- c()
oneminusspec<- c()
cutoff<- c()


for (i in 1:102) {
tp[i]<- sum(tpos[,i])
fp[i]<- sum(fpos[,i])
tn[i]<- sum(tneg[,i])
fn[i]<- sum(fneg[,i])
total<- nrow(test_3a)
accuracy[i]<- (tp[i]+tn[i])/total
misclassrate[i]<- (fp[i]+fn[i])/total
sensitivity[i]<- tp[i]/(tp[i]+fn[i])
specificity[i]<- tn[i]/(fp[i]+tn[i])
oneminusspec[i]<- fp[i]/(fp[i]+tn[i])
cutoff[i]<- 0.01*(i-1)
}




#PLOTTING ROC CURVE
plot(oneminusspec, sensitivity, type="l", lty=1, main="The Receiver 
Operating Characteristic Curve", xlab="1-Specificity", ylab="Sensitivity")

points(oneminusspec, sensitivity, pch=0) #pch=plot character, 0=square


#REPORTING MEASURES FOR THE POINT ON ROC CURVE CLOSEST TO THE IDEAL POINT (0,1)
distance<- c()
for (i in 1:102)
  distance[i]<- oneminusspec[i]^2+(1-sensitivity[i])^2

measures<- cbind(accuracy, misclassrate, sensitivity, specificity, distance, cutoff)
min.dist<- min(distance)
print(measures[which(measures[,5]==min.dist),])

#COMPUTING AREA UNDER THE ROC CURVE
sensitivity<- sensitivity[order(sensitivity)]
oneminusspec<- oneminusspec[order(oneminusspec)]


library(Hmisc) #Harrell Miscellaneous packages
lagx<- Lag(oneminusspec,shift=1)
lagy<- Lag(sensitivity, shift=1)
lagx[is.na(lagx)]<- 0
lagy[is.na(lagy)]<- 0
trapezoid<- (oneminusspec-lagx)*(sensitivity+lagy)/2
print(AUC<- sum(trapezoid))


```





## Problem 5
```{r}
concuss <- read.csv("C:/Users/saedw/OneDrive/Desktop/STAT 574 Data Mining/HW1STAT574S23/DATA SETS/concussions_data.csv")

# a) gini tree and compute micro and macro classification measures

#SPLITTING DATA INTO 80% TRAINING AND 20% TESTING SETS 
set.seed(102938)
sample <- sample(c(TRUE, FALSE), nrow(concuss),replace=TRUE, prob=c(0.8,0.2))
train<- concuss[sample,]
test<- concuss[!sample,]

#FITTING PRUNED MULTINOMIAL CLASSIFICATION TREE WITH GINI SPLITTING 
library(rpart)
tree.gini<- rpart(concussion ~ nyearsplaying + position + prevconc, 
data=train, method="class", parms=list(split="Gini"), maxdepth=4)

#PLOTTING FITTED TREE
library(rpart.plot)
rpart.plot(tree.gini, type=3)

#COMPUTING PREDICTED VALUES FOR TESTING DATA
pred.values<- predict(tree.gini, test)

#DETERMINING PREDICTED CLASSES
test<- cbind(test, pred.values)
test$maxprob<- pmax(test$'mild',test$'moderate',test$'severe')

test$predclass<- ifelse(test$maxprob==test$'mild', 'mild', 
ifelse(test$maxprob==test$'moderate','moderate', 'severe'))


#COMPUTING PERFORMANCE MEASURES FOR INDIVIDUAL CLASSES

tp<- c()
fp<- c()
tn<- c()
fn<- c()
accuracy<- c()
misclassrate<- c()
sensitivity<- c()
FNR<- c()
specificity<- c()
FPR<- c()
precision<- c()
NPV<- c()
F1score<- c()


class.metrics<- function(class) {
  
  tp.class<- ifelse(test$predclass==class & test$concussion==class,1,0)
  fp.class<- ifelse(test$predclass==class & test$concussion!=class,1,0)
  tn.class<- ifelse(test$predclass!=class & test$concussion!=class,1,0)
  fn.class<- ifelse(test$predclass!=class & test$concussion==class,1,0)

  message('CLASS MEASURES:')
  message('class:', class)
  print(paste('tp:', tp[class]<<- sum(tp.class))) 
  #<<- is global assignment, works outside the function
  print(paste('fp:', fp[class]<<- sum(fp.class)))
  print(paste('tn:', tn[class]<<- sum(tn.class)))
  print(paste('fn:', fn[class]<<- sum(fn.class)))
  total<<- nrow(test)

  print(paste('accuracy:', accuracy[class]<<- (tp[class]+tn[class])/total))
  print(paste('misclassrate:', misclassrate[class]<<- (fp[class]+fn[class])/total))
  print(paste('sensitivity:', sensitivity[class]<<- tp[class]/(tp[class]+fn[class])))
  print(paste('FNR:', FNR[class]<<- fn[class]/(tp[class]+fn[class])))
  print(paste('specificity:', specificity[class]<<- tn[class]/(fp[class]+tn[class])))
  print(paste('FPR:', FPR[class]<<- fp[class]/(fp[class]+tn[class])))
  print(paste('precision:', precision[class]<<- tp[class]/(tp[class]+fp[class])))
  print(paste('NPV:', NPV[class]<<- tn[class]/(fn[class]+tn[class])))
  print(paste('F1score:', F1score[class]<<- 2*tp[class]/(2*tp[class]+fn[class]+fp[class])))
}

class.metrics(class='moderate')
class.metrics(class='mild')
class.metrics(class='severe')


#COMPUTING MICRO MEASURES 
tp.sum<- sum(tp)
fp.sum<- sum(fp)
tn.sum<- sum(tn)
fn.sum<- sum(fn)


message('MICRO MEASURES:')
print(paste('accuracy:', accuracy.micro<- (tp.sum+tn.sum)/(tp.sum+fp.sum+tn.sum+fn.sum)))
print(paste('misclassrate:', misclassrate.micro<- (fp.sum+fn.sum)/(tp.sum+fp.sum+tn.sum+fn.sum)))
print(paste('sensitivity:', sensitivity.micro<- tp.sum/(tp.sum+fn.sum)))
print(paste('FNR:', FNR.micro<- fn.sum/(tp.sum+fn.sum)))
print(paste('specificity:', specificity.micro<- tn.sum/(fp.sum+tn.sum)))
print(paste('FPR:', FPR.micro<- fp.sum/(fp.sum+tn.sum)))
print(paste('precision:', precision.micro<- tp.sum/(tp.sum+fp.sum)))
print(paste('NPV:', NPV.micro<- tn.sum/(fn.sum+tn.sum)))
print(paste('F1-score:', F1score.micro<- 2*tp.sum/(2*tp.sum+fn.sum+fp.sum)))

#COMPUTING MACRO MEASURES
message('MACRO MEASURES:')
print(paste('accuracy:', accuracy.macro<- mean(accuracy)))
print(paste('misclassrate:', misclassrate.macro<- mean(misclassrate)))
print(paste('sensitivity:', sensitivity.macro<- mean(sensitivity)))
print(paste('FNR:', FNR.macro<- mean(FNR)))
print(paste('specificity:', specificity.macro<- mean(specificity)))
print(paste('FPR:', FPR.macro<- mean(FPR)))
print(paste('precision:', precision.macro<- mean(precision, na.rm=TRUE)))
print(paste('NPV:', NPV.macro<- mean(NPV)))
print(paste('F1-score:', F1score.macro<- mean(F1score)))

#COMPUTING WEIGHTED MACRO MEASURES
weight<- c()

for (class in 1:3) 
weight[class]<- (tp[class]+fn[class])/total

message('WEIGHTED MACRO MEASURES:')
print(paste('accuracy:', accuracy.wmacro<- weight%*%accuracy))
print(paste('misclassrate:', misclassrate.wmacro<- weight%*%misclassrate))
print(paste('sensitivity:', sensitivity.wmacro<- weight%*%sensitivity))
print(paste('FNR:', FNR.wmacro<- weight%*%FNR))
print(paste('specificity:', specificity.wmacro<- weight%*%specificity))
print(paste('FPR:', FPR.wmacro<- weight%*%FPR))
precision[is.na(precision)]<- 0
print(paste('precision:', precision.wmacro<- weight%*%precision))
print(paste('NPV:', NPV.wmacro<- weight%*%NPV))
print(paste('F1-score:', F1score.wmacro<- weight%*%F1score))





weight<- c()

for (class in 1:3) 
weight[class]<- (tp[class]+fn[class])/total

message('WEIGHTED MACRO MEASURES:')
print(paste('accuracy:', accuracy.wmacro<- weight%*%accuracy))
print(paste('misclassrate:', misclassrate.wmacro<- weight%*%misclassrate))
print(paste('sensitivity:', sensitivity.wmacro<- weight%*%sensitivity))
print(paste('FNR:', FNR.wmacro<- weight%*%FNR))
print(paste('specificity:', specificity.wmacro<- weight%*%specificity))
print(paste('FPR:', FPR.wmacro<- weight%*%FPR))
precision[is.na(precision)]<- 0
print(paste('precision:', precision.wmacro<- weight%*%precision))
print(paste('NPV:', NPV.wmacro<- weight%*%NPV))
print(paste('F1-score:', F1score.wmacro<- weight%*%F1score))

#################################################################
# (b) same as part (a) but with entropy splitting
#################################################################




tree.entropy<- rpart(concussion ~ nyearsplaying + position + prevconc, 
data=train, method="class", parms=list(split="entropy"), maxdepth=4)

#PLOTTING FITTED TREE
rpart.plot(tree.entropy, type=3)
#Note: same as tree.gini

#COMPUTING PREDICTED VALUES FOR TESTING DATA
pred.values<- predict(tree.entropy, test)

#DETERMINING PREDICTED CLASSES
test<- cbind(test, pred.values)
test$maxprob<- pmax(test$'mild',test$'moderate',test$'severe')

test$predclass<- ifelse(test$maxprob==test$'mild', 'mild', 
ifelse(test$maxprob==test$'moderate','moderate', 'severe'))


#COMPUTING PERFORMANCE MEASURES FOR INDIVIDUAL CLASSES

tp<- c()
fp<- c()
tn<- c()
fn<- c()
accuracy<- c()
misclassrate<- c()
sensitivity<- c()
FNR<- c()
specificity<- c()
FPR<- c()
precision<- c()
NPV<- c()
F1score<- c()


class.metrics<- function(class) {
  
  tp.class<- ifelse(test$predclass==class & test$concussion==class,1,0)
  fp.class<- ifelse(test$predclass==class & test$concussion!=class,1,0)
  tn.class<- ifelse(test$predclass!=class & test$concussion!=class,1,0)
  fn.class<- ifelse(test$predclass!=class & test$concussion==class,1,0)

  message('CLASS MEASURES:')
  message('class:', class)
  print(paste('tp:', tp[class]<<- sum(tp.class))) 
  #<<- is global assignment, works outside the function
  print(paste('fp:', fp[class]<<- sum(fp.class)))
  print(paste('tn:', tn[class]<<- sum(tn.class)))
  print(paste('fn:', fn[class]<<- sum(fn.class)))
  total<<- nrow(test)

  print(paste('accuracy:', accuracy[class]<<- (tp[class]+tn[class])/total))
  print(paste('misclassrate:', misclassrate[class]<<- (fp[class]+fn[class])/total))
  print(paste('sensitivity:', sensitivity[class]<<- tp[class]/(tp[class]+fn[class])))
  print(paste('FNR:', FNR[class]<<- fn[class]/(tp[class]+fn[class])))
  print(paste('specificity:', specificity[class]<<- tn[class]/(fp[class]+tn[class])))
  print(paste('FPR:', FPR[class]<<- fp[class]/(fp[class]+tn[class])))
  print(paste('precision:', precision[class]<<- tp[class]/(tp[class]+fp[class])))
  print(paste('NPV:', NPV[class]<<- tn[class]/(fn[class]+tn[class])))
  print(paste('F1score:', F1score[class]<<- 2*tp[class]/(2*tp[class]+fn[class]+fp[class])))
}

class.metrics(class='moderate')
class.metrics(class='mild')
class.metrics(class='severe')


#COMPUTING MICRO MEASURES 
tp.sum<- sum(tp)
fp.sum<- sum(fp)
tn.sum<- sum(tn)
fn.sum<- sum(fn)


message('MICRO MEASURES:')
print(paste('accuracy:', accuracy.micro<- (tp.sum+tn.sum)/(tp.sum+fp.sum+tn.sum+fn.sum)))
print(paste('misclassrate:', misclassrate.micro<- (fp.sum+fn.sum)/(tp.sum+fp.sum+tn.sum+fn.sum)))
print(paste('sensitivity:', sensitivity.micro<- tp.sum/(tp.sum+fn.sum)))
print(paste('FNR:', FNR.micro<- fn.sum/(tp.sum+fn.sum)))
print(paste('specificity:', specificity.micro<- tn.sum/(fp.sum+tn.sum)))
print(paste('FPR:', FPR.micro<- fp.sum/(fp.sum+tn.sum)))
print(paste('precision:', precision.micro<- tp.sum/(tp.sum+fp.sum)))
print(paste('NPV:', NPV.micro<- tn.sum/(fn.sum+tn.sum)))
print(paste('F1-score:', F1score.micro<- 2*tp.sum/(2*tp.sum+fn.sum+fp.sum)))

#COMPUTING MACRO MEASURES
message('MACRO MEASURES:')
print(paste('accuracy:', accuracy.macro<- mean(accuracy)))
print(paste('misclassrate:', misclassrate.macro<- mean(misclassrate)))
print(paste('sensitivity:', sensitivity.macro<- mean(sensitivity)))
print(paste('FNR:', FNR.macro<- mean(FNR)))
print(paste('specificity:', specificity.macro<- mean(specificity)))
print(paste('FPR:', FPR.macro<- mean(FPR)))
print(paste('precision:', precision.macro<- mean(precision, na.rm=TRUE)))
print(paste('NPV:', NPV.macro<- mean(NPV)))
print(paste('F1-score:', F1score.macro<- mean(F1score)))

#COMPUTING WEIGHTED MACRO MEASURES
weight<- c()

for (class in 1:3) 
weight[class]<- (tp[class]+fn[class])/total

message('WEIGHTED MACRO MEASURES:')
print(paste('accuracy:', accuracy.wmacro<- weight%*%accuracy))
print(paste('misclassrate:', misclassrate.wmacro<- weight%*%misclassrate))
print(paste('sensitivity:', sensitivity.wmacro<- weight%*%sensitivity))
print(paste('FNR:', FNR.wmacro<- weight%*%FNR))
print(paste('specificity:', specificity.wmacro<- weight%*%specificity))
print(paste('FPR:', FPR.wmacro<- weight%*%FPR))
precision[is.na(precision)]<- 0
print(paste('precision:', precision.wmacro<- weight%*%precision))
print(paste('NPV:', NPV.wmacro<- weight%*%NPV))
print(paste('F1-score:', F1score.wmacro<- weight%*%F1score))





weight<- c()

for (class in 1:3) 
weight[class]<- (tp[class]+fn[class])/total

message('WEIGHTED MACRO MEASURES:')
print(paste('accuracy:', accuracy.wmacro<- weight%*%accuracy))
print(paste('misclassrate:', misclassrate.wmacro<- weight%*%misclassrate))
print(paste('sensitivity:', sensitivity.wmacro<- weight%*%sensitivity))
print(paste('FNR:', FNR.wmacro<- weight%*%FNR))
print(paste('specificity:', specificity.wmacro<- weight%*%specificity))
print(paste('FPR:', FPR.wmacro<- weight%*%FPR))
precision[is.na(precision)]<- 0
print(paste('precision:', precision.wmacro<- weight%*%precision))
print(paste('NPV:', NPV.wmacro<- weight%*%NPV))
print(paste('F1-score:', F1score.wmacro<- weight%*%F1score))





########################################################################
# c) chaid splitting criterion same as a and b
########################################################################


#FITTING PRUNED MULTINOMIAL CLASSIFICATION TREE WITH CHAID SPLITTING  
#BINNING CONTINUOUS PREDICTOR VARIABLES 
library(dplyr)

concuss <- read.csv("C:/Users/saedw/OneDrive/Desktop/STAT 574 Data Mining/HW1STAT574S23/DATA SETS/concussions_data.csv")
concuss.data <- mutate(concuss, age.cat=ntile(age,10))



#SPLITTING DATA INTO 80% TRAINING AND 20% TESTING SETS  
set.seed(566222)
sample <- sample(c(TRUE, FALSE), nrow(concuss.data), 
replace=TRUE, prob=c(0.8,0.2))
train<- concuss.data[sample,]
test<- concuss.data[!sample,]

#FITTING MULTINOMIAL CLASSIFICATION TREE WITH CHAID SPLITTING
library(CHAID)
tree.CHAID<- chaid(as.factor(concussion) ~ as.factor(age.cat) + as.factor(nyearsplaying) 
+ as.factor(position) + as.factor(prevconc), data=train)

#PLOTTING FITTED TREE
plot(tree.CHAID, type="simple")

#COMPUTING PREDICTED VALUES FOR TESTING DATA 
pred.values<- predict(tree.CHAID, newdata=test)

#COMPUTING PERFORMANCE MEASURES FOR FITTED CHAID TREE
test$predclass<- pred.values

View(test)
#DETERMINING PREDICTED CLASSES
test<- cbind(test, pred.values)
test$maxprob<- pmax(test$'mild',test$'moderate',test$'severe')

test$predclass<- ifelse(test$maxprob==test$'mild', 'mild', 
ifelse(test$maxprob==test$'moderate','moderate', 'severe'))


#COMPUTING PERFORMANCE MEASURES FOR INDIVIDUAL CLASSES

tp<- c()
fp<- c()
tn<- c()
fn<- c()
accuracy<- c()
misclassrate<- c()
sensitivity<- c()
FNR<- c()
specificity<- c()
FPR<- c()
precision<- c()
NPV<- c()
F1score<- c()


class.metrics<- function(class) {
  
  tp.class<- ifelse(test$predclass==class & test$concussion==class,1,0)
  fp.class<- ifelse(test$predclass==class & test$concussion!=class,1,0)
  tn.class<- ifelse(test$predclass!=class & test$concussion!=class,1,0)
  fn.class<- ifelse(test$predclass!=class & test$concussion==class,1,0)

  message('CLASS MEASURES:')
  message('class:', class)
  print(paste('tp:', tp[class]<<- sum(tp.class))) 
  #<<- is global assignment, works outside the function
  print(paste('fp:', fp[class]<<- sum(fp.class)))
  print(paste('tn:', tn[class]<<- sum(tn.class)))
  print(paste('fn:', fn[class]<<- sum(fn.class)))
  total<<- nrow(test)

  print(paste('accuracy:', accuracy[class]<<- (tp[class]+tn[class])/total))
  print(paste('misclassrate:', misclassrate[class]<<- (fp[class]+fn[class])/total))
  print(paste('sensitivity:', sensitivity[class]<<- tp[class]/(tp[class]+fn[class])))
  print(paste('FNR:', FNR[class]<<- fn[class]/(tp[class]+fn[class])))
  print(paste('specificity:', specificity[class]<<- tn[class]/(fp[class]+tn[class])))
  print(paste('FPR:', FPR[class]<<- fp[class]/(fp[class]+tn[class])))
  print(paste('precision:', precision[class]<<- tp[class]/(tp[class]+fp[class])))
  print(paste('NPV:', NPV[class]<<- tn[class]/(fn[class]+tn[class])))
  print(paste('F1score:', F1score[class]<<- 2*tp[class]/(2*tp[class]+fn[class]+fp[class])))
}

class.metrics(class='moderate')
class.metrics(class='mild')
class.metrics(class='severe')


#COMPUTING MICRO MEASURES 
tp.sum<- sum(tp)
fp.sum<- sum(fp)
tn.sum<- sum(tn)
fn.sum<- sum(fn)


message('MICRO MEASURES:')
print(paste('accuracy:', accuracy.micro<- (tp.sum+tn.sum)/(tp.sum+fp.sum+tn.sum+fn.sum)))
print(paste('misclassrate:', misclassrate.micro<- (fp.sum+fn.sum)/(tp.sum+fp.sum+tn.sum+fn.sum)))
print(paste('sensitivity:', sensitivity.micro<- tp.sum/(tp.sum+fn.sum)))
print(paste('FNR:', FNR.micro<- fn.sum/(tp.sum+fn.sum)))
print(paste('specificity:', specificity.micro<- tn.sum/(fp.sum+tn.sum)))
print(paste('FPR:', FPR.micro<- fp.sum/(fp.sum+tn.sum)))
print(paste('precision:', precision.micro<- tp.sum/(tp.sum+fp.sum)))
print(paste('NPV:', NPV.micro<- tn.sum/(fn.sum+tn.sum)))
print(paste('F1-score:', F1score.micro<- 2*tp.sum/(2*tp.sum+fn.sum+fp.sum)))

#COMPUTING MACRO MEASURES
message('MACRO MEASURES:')
print(paste('accuracy:', accuracy.macro<- mean(accuracy)))
print(paste('misclassrate:', misclassrate.macro<- mean(misclassrate)))
print(paste('sensitivity:', sensitivity.macro<- mean(sensitivity)))
print(paste('FNR:', FNR.macro<- mean(FNR)))
print(paste('specificity:', specificity.macro<- mean(specificity)))
print(paste('FPR:', FPR.macro<- mean(FPR)))
print(paste('precision:', precision.macro<- mean(precision, na.rm=TRUE)))
print(paste('NPV:', NPV.macro<- mean(NPV)))
print(paste('F1-score:', F1score.macro<- mean(F1score)))

#COMPUTING WEIGHTED MACRO MEASURES
weight<- c()

for (class in 1:3) 
weight[class]<- (tp[class]+fn[class])/total

message('WEIGHTED MACRO MEASURES:')
print(paste('accuracy:', accuracy.wmacro<- weight%*%accuracy))
print(paste('misclassrate:', misclassrate.wmacro<- weight%*%misclassrate))
print(paste('sensitivity:', sensitivity.wmacro<- weight%*%sensitivity))
print(paste('FNR:', FNR.wmacro<- weight%*%FNR))
print(paste('specificity:', specificity.wmacro<- weight%*%specificity))
print(paste('FPR:', FPR.wmacro<- weight%*%FPR))
precision[is.na(precision)]<- 0
print(paste('precision:', precision.wmacro<- weight%*%precision))
print(paste('NPV:', NPV.wmacro<- weight%*%NPV))
print(paste('F1-score:', F1score.wmacro<- weight%*%F1score))





weight<- c()

for (class in 1:3) 
weight[class]<- (tp[class]+fn[class])/total

message('WEIGHTED MACRO MEASURES:')
print(paste('accuracy:', accuracy.wmacro<- weight%*%accuracy))
print(paste('misclassrate:', misclassrate.wmacro<- weight%*%misclassrate))
print(paste('sensitivity:', sensitivity.wmacro<- weight%*%sensitivity))
print(paste('FNR:', FNR.wmacro<- weight%*%FNR))
print(paste('specificity:', specificity.wmacro<- weight%*%specificity))
print(paste('FPR:', FPR.wmacro<- weight%*%FPR))
precision[is.na(precision)]<- 0
print(paste('precision:', precision.wmacro<- weight%*%precision))
print(paste('NPV:', NPV.wmacro<- weight%*%NPV))
print(paste('F1-score:', F1score.wmacro<- weight%*%F1score))


```


